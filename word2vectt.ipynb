{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import spacy  # Pour la lemmatisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df_csv = pd.read_csv(\"questions_dataset.csv\", header=None, names=[\"data\"])\n",
    "\n",
    "# Prepare an empty list to store transformed rows\n",
    "transformed_data = []\n",
    "\n",
    "# Iterate through each row\n",
    "for _, row in df_csv.iterrows():\n",
    "    # Split category into Category and Subcategory\n",
    "    category_parts = row['data'].split(\":\")\n",
    "    classe=category_parts[0]\n",
    "    question_split=category_parts[1]\n",
    "    # Split Question into Label and the rest\n",
    "    label_question= question_split.split(' ')\n",
    "    label = label_question[0]  \n",
    "    question_rest =  \" \".join(label_question[1:])  # The rest as the question  # Remaining part of the question\n",
    "    \n",
    "    # Append the transformed data as a tuple\n",
    "    transformed_data.append(( question_rest,classe, label))\n",
    "\n",
    "# Create a new DataFrame with the transformed data\n",
    "df = pd.DataFrame(transformed_data, columns=[\"Question\",\"Category\", \"Subcategory\"])\n",
    "\n",
    "# Save the transformed DataFrame to a new CSV file\n",
    "df.to_csv(\"csv_data_file.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Question'] = df['Question'].str.lower()  # Convertir le texte en minuscules\n",
    "df['Question'] = df['Question'].str.replace(r'[^\\w\\s]', '', regex=True)  # Supprimer les ponctuations\n",
    "df['Question'] = df['Question'].str.strip()  # Supprimer les espaces inutiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Question'], df['Category'], test_size=0.2, random_state=42)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Tokenization et token de WH questions\n",
    "def tokenize_and_process(text):\n",
    "    tokens = nlp(text)\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.is_alpha and not token.is_stop:\n",
    "            if token.tag_ in [\"WP\", \"WDT\", \"WP$\", \"WRB\"]:  # WH questions POS tags\n",
    "                processed_tokens.append(token.text.lower())  # Garder les WH tels quels\n",
    "            else:\n",
    "                lemmatized_token = token.lemma_.lower()  # Lemmatization pour les autres mots\n",
    "                processed_tokens.append(lemmatized_token)\n",
    "    return processed_tokens\n",
    "\n",
    "# Formation du modèle Word2Vec\n",
    "def train_word2vec(corpus, vector_size=100, window=4, min_count=5, workers=4):\n",
    "    sentences = [tokenize_and_process(text) for text in corpus]\n",
    "    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
    "    return model\n",
    "\n",
    "# Transformer les questions en vecteurs avec Word2Vec\n",
    "def vectorize_with_word2vec(texts, model, size=100):\n",
    "    vectorized_texts = []\n",
    "    for text in texts:\n",
    "        tokens = tokenize_and_process(text)\n",
    "        vectors = [model.wv[token] for token in tokens if token in model.wv]  # Garde uniquement les mots présents dans le modèle\n",
    "        if vectors:\n",
    "            vectorized_texts.append(np.mean(vectors, axis=0))  # Moyenne des vecteurs\n",
    "        else:\n",
    "            vectorized_texts.append(np.zeros(size))  # Remplacer par un vecteur zéro si aucun token trouvé\n",
    "    return np.array(vectorized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Word2Vec: 0.28780934922089824\n"
     ]
    }
   ],
   "source": [
    "# Entraînement du modèle Word2Vec\n",
    "word2vec_model = train_word2vec(X_train, vector_size=100, window=4, min_count=5, workers=4)\n",
    "\n",
    "# Vectoriser le train et test set\n",
    "X_train_vectors = vectorize_with_word2vec(X_train, word2vec_model)\n",
    "X_test_vectors = vectorize_with_word2vec(X_test, word2vec_model)\n",
    "\n",
    "# Classification avec Logistic Regression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Prédictions et évaluation\n",
    "y_pred = classifier.predict(X_test_vectors)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using Word2Vec: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Encode les labels (les catégories)\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# Définir le modèle ANN\n",
    "def build_ann_model(input_size, hidden_layer_sizes, output_size, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_sizes[0], input_dim=input_size, activation=activation))\n",
    "    for units in hidden_layer_sizes[1:]:\n",
    "        model.add(Dense(units, activation=activation))\n",
    "    model.add(Dense(output_size, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Configuration des hyperparamètres\n",
    "input_size = 100  # taille des vecteurs générés par Word2Vec\n",
    "hidden_layer_sizes = [128, 64]  # Taille des couches cachées\n",
    "output_size = len(label_encoder.classes_)  # Nombre d'étiquettes uniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alaed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2235 - loss: 1.7540 - val_accuracy: 0.2222 - val_loss: 1.6560\n",
      "Epoch 2/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2538 - loss: 1.6436 - val_accuracy: 0.2314 - val_loss: 1.6313\n",
      "Epoch 3/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2603 - loss: 1.6302 - val_accuracy: 0.2658 - val_loss: 1.6132\n",
      "Epoch 4/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2903 - loss: 1.6061 - val_accuracy: 0.2955 - val_loss: 1.5979\n",
      "Epoch 5/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3185 - loss: 1.5838 - val_accuracy: 0.2864 - val_loss: 1.5871\n",
      "Epoch 6/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3296 - loss: 1.5704 - val_accuracy: 0.3013 - val_loss: 1.5752\n",
      "Epoch 7/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3163 - loss: 1.5696 - val_accuracy: 0.3276 - val_loss: 1.5601\n",
      "Epoch 8/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3463 - loss: 1.5462 - val_accuracy: 0.3677 - val_loss: 1.5565\n",
      "Epoch 9/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3675 - loss: 1.5342 - val_accuracy: 0.3608 - val_loss: 1.5298\n",
      "Epoch 10/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3635 - loss: 1.5206 - val_accuracy: 0.3666 - val_loss: 1.5091\n",
      "Epoch 11/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3954 - loss: 1.4873 - val_accuracy: 0.3734 - val_loss: 1.4979\n",
      "Epoch 12/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4252 - loss: 1.4539 - val_accuracy: 0.3711 - val_loss: 1.4856\n",
      "Epoch 13/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3909 - loss: 1.4592 - val_accuracy: 0.3998 - val_loss: 1.4824\n",
      "Epoch 14/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4107 - loss: 1.4388 - val_accuracy: 0.3814 - val_loss: 1.4632\n",
      "Epoch 15/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4344 - loss: 1.4090 - val_accuracy: 0.4353 - val_loss: 1.4443\n",
      "Epoch 16/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4583 - loss: 1.3800 - val_accuracy: 0.4124 - val_loss: 1.4445\n",
      "Epoch 17/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4451 - loss: 1.3961 - val_accuracy: 0.4467 - val_loss: 1.4359\n",
      "Epoch 18/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4774 - loss: 1.3628 - val_accuracy: 0.4215 - val_loss: 1.4300\n",
      "Epoch 19/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4553 - loss: 1.3748 - val_accuracy: 0.4341 - val_loss: 1.4252\n",
      "Epoch 20/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4614 - loss: 1.3758 - val_accuracy: 0.4444 - val_loss: 1.4173\n",
      "Epoch 21/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4614 - loss: 1.3587 - val_accuracy: 0.4490 - val_loss: 1.4149\n",
      "Epoch 22/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4746 - loss: 1.3404 - val_accuracy: 0.4467 - val_loss: 1.4159\n",
      "Epoch 23/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4802 - loss: 1.3318 - val_accuracy: 0.4341 - val_loss: 1.4105\n",
      "Epoch 24/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4823 - loss: 1.3165 - val_accuracy: 0.4284 - val_loss: 1.4099\n",
      "Epoch 25/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4897 - loss: 1.3197 - val_accuracy: 0.4479 - val_loss: 1.4075\n",
      "Epoch 26/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4948 - loss: 1.2974 - val_accuracy: 0.4502 - val_loss: 1.4011\n",
      "Epoch 27/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5031 - loss: 1.2947 - val_accuracy: 0.4593 - val_loss: 1.3962\n",
      "Epoch 28/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5036 - loss: 1.2989 - val_accuracy: 0.4616 - val_loss: 1.3946\n",
      "Epoch 29/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4988 - loss: 1.2921 - val_accuracy: 0.4593 - val_loss: 1.3981\n",
      "Epoch 30/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5113 - loss: 1.2790 - val_accuracy: 0.4570 - val_loss: 1.3959\n",
      "Epoch 31/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5169 - loss: 1.2753 - val_accuracy: 0.4674 - val_loss: 1.3936\n",
      "Epoch 32/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5055 - loss: 1.2935 - val_accuracy: 0.4593 - val_loss: 1.4005\n",
      "Epoch 33/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5133 - loss: 1.2767 - val_accuracy: 0.4651 - val_loss: 1.4012\n",
      "Epoch 34/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5067 - loss: 1.3048 - val_accuracy: 0.4582 - val_loss: 1.3894\n",
      "Epoch 35/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5032 - loss: 1.2636 - val_accuracy: 0.4593 - val_loss: 1.3876\n",
      "Epoch 36/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5159 - loss: 1.2598 - val_accuracy: 0.4662 - val_loss: 1.3911\n",
      "Epoch 37/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5230 - loss: 1.2496 - val_accuracy: 0.4685 - val_loss: 1.3938\n",
      "Epoch 38/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5097 - loss: 1.2684 - val_accuracy: 0.4708 - val_loss: 1.3847\n",
      "Epoch 39/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5182 - loss: 1.2459 - val_accuracy: 0.4674 - val_loss: 1.3890\n",
      "Epoch 40/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5255 - loss: 1.2464 - val_accuracy: 0.4765 - val_loss: 1.3835\n",
      "Epoch 41/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5232 - loss: 1.2481 - val_accuracy: 0.4605 - val_loss: 1.3847\n",
      "Epoch 42/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5248 - loss: 1.2384 - val_accuracy: 0.4628 - val_loss: 1.3835\n",
      "Epoch 43/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5343 - loss: 1.2258 - val_accuracy: 0.4685 - val_loss: 1.3931\n",
      "Epoch 44/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5449 - loss: 1.2051 - val_accuracy: 0.4628 - val_loss: 1.3969\n",
      "Epoch 45/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5291 - loss: 1.2339 - val_accuracy: 0.4731 - val_loss: 1.3795\n",
      "Epoch 46/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5171 - loss: 1.2300 - val_accuracy: 0.4593 - val_loss: 1.4028\n",
      "Epoch 47/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5373 - loss: 1.2190 - val_accuracy: 0.4719 - val_loss: 1.3834\n",
      "Epoch 48/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5288 - loss: 1.2205 - val_accuracy: 0.4800 - val_loss: 1.3761\n",
      "Epoch 49/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5338 - loss: 1.2301 - val_accuracy: 0.4822 - val_loss: 1.3796\n",
      "Epoch 50/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5363 - loss: 1.2151 - val_accuracy: 0.4742 - val_loss: 1.3761\n",
      "Epoch 51/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5337 - loss: 1.2378 - val_accuracy: 0.4708 - val_loss: 1.3787\n",
      "Epoch 52/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5452 - loss: 1.2222 - val_accuracy: 0.4685 - val_loss: 1.3757\n",
      "Epoch 53/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5464 - loss: 1.1945 - val_accuracy: 0.4765 - val_loss: 1.3747\n",
      "Epoch 54/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5550 - loss: 1.1882 - val_accuracy: 0.4811 - val_loss: 1.3814\n",
      "Epoch 55/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5525 - loss: 1.1867 - val_accuracy: 0.4777 - val_loss: 1.3712\n",
      "Epoch 56/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5531 - loss: 1.1946 - val_accuracy: 0.4662 - val_loss: 1.3714\n",
      "Epoch 57/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5608 - loss: 1.1903 - val_accuracy: 0.4937 - val_loss: 1.3737\n",
      "Epoch 58/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5616 - loss: 1.1750 - val_accuracy: 0.4834 - val_loss: 1.3796\n",
      "Epoch 59/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5731 - loss: 1.1459 - val_accuracy: 0.4731 - val_loss: 1.3798\n",
      "Epoch 60/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5603 - loss: 1.1710 - val_accuracy: 0.4834 - val_loss: 1.3675\n",
      "Epoch 61/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5630 - loss: 1.1571 - val_accuracy: 0.4914 - val_loss: 1.3771\n",
      "Epoch 62/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5775 - loss: 1.1314 - val_accuracy: 0.4845 - val_loss: 1.3678\n",
      "Epoch 63/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5479 - loss: 1.1825 - val_accuracy: 0.4880 - val_loss: 1.3659\n",
      "Epoch 64/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5747 - loss: 1.1522 - val_accuracy: 0.4731 - val_loss: 1.3966\n",
      "Epoch 65/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5614 - loss: 1.1629 - val_accuracy: 0.4960 - val_loss: 1.3743\n",
      "Epoch 66/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5721 - loss: 1.1403 - val_accuracy: 0.4903 - val_loss: 1.3640\n",
      "Epoch 67/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5675 - loss: 1.1559 - val_accuracy: 0.4891 - val_loss: 1.3665\n",
      "Epoch 68/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5645 - loss: 1.1701 - val_accuracy: 0.4822 - val_loss: 1.3633\n",
      "Epoch 69/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5712 - loss: 1.1307 - val_accuracy: 0.4822 - val_loss: 1.3656\n",
      "Epoch 70/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5763 - loss: 1.1273 - val_accuracy: 0.4891 - val_loss: 1.3649\n",
      "Epoch 71/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5700 - loss: 1.1436 - val_accuracy: 0.4857 - val_loss: 1.3613\n",
      "Epoch 72/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5811 - loss: 1.1244 - val_accuracy: 0.4834 - val_loss: 1.3758\n",
      "Epoch 73/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5799 - loss: 1.1234 - val_accuracy: 0.4868 - val_loss: 1.3635\n",
      "Epoch 74/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5773 - loss: 1.1319 - val_accuracy: 0.4845 - val_loss: 1.3745\n",
      "Epoch 75/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5812 - loss: 1.1237 - val_accuracy: 0.4731 - val_loss: 1.3821\n",
      "Epoch 76/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5878 - loss: 1.1220 - val_accuracy: 0.4994 - val_loss: 1.3658\n",
      "Epoch 77/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5808 - loss: 1.1111 - val_accuracy: 0.4834 - val_loss: 1.3619\n",
      "Epoch 78/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5832 - loss: 1.1135 - val_accuracy: 0.4857 - val_loss: 1.3682\n",
      "Epoch 79/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6046 - loss: 1.1077 - val_accuracy: 0.4891 - val_loss: 1.3658\n",
      "Epoch 80/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5969 - loss: 1.0844 - val_accuracy: 0.4811 - val_loss: 1.3600\n",
      "Epoch 81/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6022 - loss: 1.0865 - val_accuracy: 0.4914 - val_loss: 1.3598\n",
      "Epoch 82/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5961 - loss: 1.0915 - val_accuracy: 0.4937 - val_loss: 1.3635\n",
      "Epoch 83/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5957 - loss: 1.0891 - val_accuracy: 0.4960 - val_loss: 1.3577\n",
      "Epoch 84/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6053 - loss: 1.0786 - val_accuracy: 0.4914 - val_loss: 1.3629\n",
      "Epoch 85/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5959 - loss: 1.0955 - val_accuracy: 0.5017 - val_loss: 1.3619\n",
      "Epoch 86/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6073 - loss: 1.0657 - val_accuracy: 0.4891 - val_loss: 1.3884\n",
      "Epoch 87/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5861 - loss: 1.0978 - val_accuracy: 0.4983 - val_loss: 1.3572\n",
      "Epoch 88/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5900 - loss: 1.0864 - val_accuracy: 0.4857 - val_loss: 1.3786\n",
      "Epoch 89/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6157 - loss: 1.0533 - val_accuracy: 0.4960 - val_loss: 1.3617\n",
      "Epoch 90/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6012 - loss: 1.0816 - val_accuracy: 0.4891 - val_loss: 1.3841\n",
      "Epoch 91/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6261 - loss: 1.0645 - val_accuracy: 0.5029 - val_loss: 1.3524\n",
      "Epoch 92/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6097 - loss: 1.0645 - val_accuracy: 0.4914 - val_loss: 1.3687\n",
      "Epoch 93/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6220 - loss: 1.0364 - val_accuracy: 0.5086 - val_loss: 1.3556\n",
      "Epoch 94/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6073 - loss: 1.0670 - val_accuracy: 0.4994 - val_loss: 1.3766\n",
      "Epoch 95/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6211 - loss: 1.0476 - val_accuracy: 0.4800 - val_loss: 1.3659\n",
      "Epoch 96/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6165 - loss: 1.0445 - val_accuracy: 0.5029 - val_loss: 1.3585\n",
      "Epoch 97/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6253 - loss: 1.0303 - val_accuracy: 0.4994 - val_loss: 1.3610\n",
      "Epoch 98/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6265 - loss: 1.0353 - val_accuracy: 0.4948 - val_loss: 1.3664\n",
      "Epoch 99/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6183 - loss: 1.0358 - val_accuracy: 0.4926 - val_loss: 1.3623\n",
      "Epoch 100/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6296 - loss: 1.0023 - val_accuracy: 0.4937 - val_loss: 1.3628\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Test set accuracy with ANN: 0.5133\n"
     ]
    }
   ],
   "source": [
    "# Construction du modèle ANN\n",
    "ann_model = build_ann_model(input_size, hidden_layer_sizes, output_size)\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = ann_model.fit(X_train_vectors, y_train_encoded, epochs=100, batch_size=64, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Prédictions sur le jeu de test\n",
    "y_pred_ann = ann_model.predict(X_test_vectors)\n",
    "y_pred_classes_ann = np.argmax(y_pred_ann, axis=1)\n",
    "\n",
    "# Évaluation du modèle\n",
    "accuracy_ann = accuracy_score(y_test_encoded, y_pred_classes_ann)\n",
    "print(f\"Test set accuracy with ANN: {accuracy_ann:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step\n",
      "Test set accuracy with ANN: 0.5133\n",
      "Classification Report:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ABBR': {'precision': 0.7692307692307693,\n",
       "  'recall': 0.43478260869565216,\n",
       "  'f1-score': 0.5555555555555556,\n",
       "  'support': 23.0},\n",
       " 'DESC': {'precision': 0.5060728744939271,\n",
       "  'recall': 0.5506607929515418,\n",
       "  'f1-score': 0.5274261603375527,\n",
       "  'support': 227.0},\n",
       " 'ENTY': {'precision': 0.4954545454545455,\n",
       "  'recall': 0.44308943089430897,\n",
       "  'f1-score': 0.4678111587982833,\n",
       "  'support': 246.0},\n",
       " 'HUM': {'precision': 0.43125,\n",
       "  'recall': 0.575,\n",
       "  'f1-score': 0.4928571428571429,\n",
       "  'support': 240.0},\n",
       " 'LOC': {'precision': 0.6447368421052632,\n",
       "  'recall': 0.5903614457831325,\n",
       "  'f1-score': 0.6163522012578616,\n",
       "  'support': 166.0},\n",
       " 'NUM': {'precision': 0.5755395683453237,\n",
       "  'recall': 0.42328042328042326,\n",
       "  'f1-score': 0.4878048780487805,\n",
       "  'support': 189.0},\n",
       " 'accuracy': 0.5132905591200734,\n",
       " 'macro avg': {'precision': 0.5703807666049715,\n",
       "  'recall': 0.5028624502675099,\n",
       "  'f1-score': 0.5246345161425294,\n",
       "  'support': 1091.0},\n",
       " 'weighted avg': {'precision': 0.5258991407800067,\n",
       "  'recall': 0.5132905591200734,\n",
       "  'f1-score': 0.5136391960444718,\n",
       "  'support': 1091.0}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Prédictions sur le jeu de test\n",
    "y_pred_ann = ann_model.predict(X_test_vectors)\n",
    "y_pred_classes_ann = np.argmax(y_pred_ann, axis=1)\n",
    "\n",
    "# Evaluation du modèle ANN\n",
    "accuracy_ann = accuracy_score(y_test_encoded, y_pred_classes_ann)\n",
    "print(f\"Test set accuracy with ANN: {accuracy_ann:.4f}\")\n",
    "\n",
    "# Générer le rapport de classification\n",
    "target_names = label_encoder.classes_  # Obtenir les noms des catégories\n",
    "report_ann = classification_report(y_test_encoded, y_pred_classes_ann, target_names=target_names, output_dict=True)\n",
    "\n",
    "# Rapport final\n",
    "print(\"Classification Report:\\n\")\n",
    "report_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
